# BioAI Training Strategy Guide ðŸ§ 

**Version:** 0.5.1 (Industrial Beta)

Wie wird ein BioAI-Agent intelligent? Wir nutzen ein erweitertes **4-Ebenen-Modell**, das biologische Prinzipien mit technischer PrÃ¤zision und industrieller Sicherheit vereint.

---

## Ebene 1: Instinkte (Injected Knowledge)
Angeborenes Wissen, das ab Sekunde 0 verfÃ¼gbar ist. Dies ist die Basis fÃ¼r **Safety First**.

* **Konzept:** Harte, deterministische Regeln, die direkt im LangzeitgedÃ¤chtnis (LTM) verankert sind. Sie benÃ¶tigen kein Training.
* **Code:** `API_Teach(brain, input, action, 1.0f)`
* **Anwendung:**
    * **Sicherheit (Hard Safety):** "Wenn Sensor > 100Â°C, dann NOT-AUS." (Gewicht 1.0 = Unverhandelbares Gesetz).
    * **Bias (Soft Safety):** "Wenn du nicht weiter weiÃŸt, dreh dich nach rechts." (Gewicht 0.3 = Tendenz).
* **Vorteil:** Auditierbar. Der TÃœV kann den Quellcode prÃ¼fen und sehen: Diese Regel ist fest im System.

---

## Ebene 2: Erfahrung (Reinforcement Learning)
Lernen durch Versuch und Irrtum im laufenden Betrieb.

* **Konzept:** Der Agent probiert etwas aus. Basierend auf dem Ergebnis (Reward/Punishment) wird die Verbindung angepasst.
* **Code:** `API_Feedback(brain, reward, action)`
* **Prozess:**
    1.  Agent handelt (`API_Update`).
    2.  Sensor misst Ergebnis (z.B. "Temperatur gesunken").
    3.  System gibt Feedback:
        * **Positiv (+1.0):** Verbindung wird verstÃ¤rkt.
        * **Negativ (-1.0):** Verbindung wird gehemmt.
* **Speicher-Schutz (RauschunterdrÃ¼ckung):** BioAI nutzt ein KurzzeitgedÃ¤chtnis (STM). Nur wenn eine Erfahrung mehrfach bestÃ¤tigt wird (`LTM_CONSOLIDATE_HITS`), wandert sie ins permanente GedÃ¤chtnis (LTM). Das verhindert, dass ZufÃ¤lle ("Rauschen") gelernt werden.

---

## Ebene 3: Imagination (Simulation & Planung)
Die FÃ¤higkeit, Konsequenzen vorherzusehen, *bevor* man handelt.

* **Konzept:** Der Agent nutzt sein gelerntes KausalitÃ¤ts-Wissen, um die Zukunft virtuell zu simulieren.
* **Code:** `API_Simulate(brain, inputs, count, depth)`
* **Anwendung:**
    * Ein Roboter steht vor einem Abgrund.
    * Statt zu springen (und zerstÃ¶rt zu werden), simuliert er den Sprung.
    * Ergebnis der Simulation (Tiefe 2): "Schmerz/Tod".
    * Entscheidung: Er bleibt stehen.
* **Safety:** Die Simulationstiefe ist durch `MAX_SIM_DEPTH` begrenzt, um Stack-Overflows auf IoT-GerÃ¤ten physikalisch auszuschlieÃŸen.

---

## Ebene 4: Schwarm-Wissen (Social Propagation)
Wissen ist nicht an einen einzelnen Agenten gebunden.

* **Konzept:** Da `TokenID` (64-Bit Hash) universell ist, kÃ¶nnen Erfahrungen mathematisch geteilt werden.
* **Mechanik:**
    1.  Agent A macht einen Fehler: "Input X -> Aktion Y fÃ¼hrt zu Schaden."
    2.  Agent A sendet `TokenID(X)` und `TokenID(Y)` an Agent B (Ã¼ber WLAN/LoRa).
    3.  Agent B injiziert dieses Wissen: `API_Teach(X, Y, -1.0)`.
* **Ergebnis:** Der gesamte Schwarm lernt aus dem Fehler eines einzelnen Individuums, ohne ihn selbst machen zu mÃ¼ssen ("Fleet Learning").

---

**BrainAI** - *Intelligence everywhere.*
Developed by **Sascha A. KÃ¶hne (winemp83)**
Product: **BioAI v0.5.1 (Industrial Beta)**
ðŸ“§ [koehne83@googlemail.com](mailto:koehne83@googlemail.com)

&copy; 2025 BrainAI / Sascha A. KÃ¶hne. All rights reserved.
